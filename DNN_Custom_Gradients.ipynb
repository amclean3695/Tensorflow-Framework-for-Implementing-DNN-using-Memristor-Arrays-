{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amcle\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_mapping(theta):\n",
    "    theta = 1.0 + np.divide(spice_params['K'], 1.0 + np.exp(-theta))\n",
    "    sum_theta = np.sum(theta, axis=1)\n",
    "    g = np.divide(theta, np.tile(sum_theta, (theta.shape[1], 1)).transpose())\n",
    "    # np.tile repeats the row 394 times then transposes to get the right dimension\n",
    "    return g.astype(np.float32)\n",
    "\n",
    "def sigmoid(z):\n",
    "    half_Vdd = -spice_params['Vdd'] / 2.0\n",
    "    sharp_factor = spice_params['cc'][3]\n",
    "    s_param = spice_params['cc'][2]\n",
    "    \n",
    "    return np.multiply(half_Vdd, np.tanh(np.multiply(sharp_factor, np.subtract(z, s_param))))\n",
    "\n",
    "def sigmoidn(z):\n",
    "    half_Vdd = spice_params['Vdd'] / 2.0\n",
    "    sharp_factor = spice_params['cc2'][3]\n",
    "    s_param = spice_params['cc2'][2]\n",
    "    \n",
    "    return np.multiply(half_Vdd, np.tanh(np.multiply(sharp_factor, np.subtract(z, s_param))))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    grad = np.multiply(-spice_params['cc'][3], np.subtract(1, np.square(np.tanh(np.multiply(spice_params['cc'][3], \\\n",
    "                                                                        np.subtract(z, spice_params['cc'][2]))))))\n",
    "    return grad\n",
    "\n",
    "def sigmoidGradientn(z):\n",
    "    grad_n = np.multiply(spice_params['cc2'][3], np.subtract(1, np.square(np.tanh(np.multiply(spice_params['cc2'][3], \\\n",
    "                                                                        np.subtract(z, spice_params['cc2'][2]))))))\n",
    "    return grad_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is not really considered a function we should care about for backprop to work correctly \n",
    "def randInitializeWeights(L_in,L_out):\n",
    "    '''Function that randomly initializes the weights of a layer with L_in incoming connections\n",
    "    and L_out outgoing connections as implemented in randInitializeWeights.m\n",
    "    Note that W should be set to a matrix of size(L_out, 1 + L_in) as the column row of W handles \n",
    "    the \"bias\" terms. Duplicates functionality from randInitializeWeights.m \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in := tensor, number of incoming connections\n",
    "    L_out := tensor, number of outgoing connections\n",
    "    Results returns a tensor with of initial weights for the network'''\n",
    "    \n",
    "    epsilon = np.sqrt(6/(L_in+L_out))\n",
    "    return tf.random_uniform([L_out, L_in + 1], dtype = tf.float32, minval = -np.sqrt(6/(L_in+L_out)), maxval = np.sqrt(6/(L_in+L_out)))\n",
    "    \n",
    "def weight_md(theta):\n",
    "    '''Implements physical circuit constraints on the weight mapping that occurs during backpropogation of the network. This\n",
    "    function duplicates the functionality of the weight_md.m file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta := unconstrained weights matrix\n",
    "    K := ùêæ is a constant number which is the difference between the ùúéùëöùëéùë• and ùúéùëöùëñùëõ. \n",
    "    Results\n",
    "    ---------\n",
    "    returns activated neuron output\n",
    "    ''' \n",
    "    num_pn = theta.get_shape().as_list()[0]\n",
    "    num_n = theta.get_shape().as_list()[1]\n",
    "    g1 = tf.multiply(spice_params['K'],tf.multiply(tf.divide(1.0,tf.add(1.0,tf.exp(-theta))), \\\n",
    "                                                   tf.subtract(1.0,tf.divide(1.0,tf.add(1.0,tf.exp(-theta))))))\n",
    "    g1 = tf.transpose(tf.contrib.kfac.utils.kronecker_product(g1,tf.ones((num_n, 1))))\n",
    "    theta = tf.add(1.0, tf.divide(spice_params['K'],tf.add(1.0,tf.exp(-theta))))\n",
    "    sum_theta = tf.reduce_sum(theta, 1, keepdims = True)\n",
    "    sums_theta1 = tf.tile(sum_theta,(1,num_n))\n",
    "    theta = tf.transpose(tf.contrib.kfac.utils.kronecker_product(theta,tf.ones((num_n, 1))))\n",
    "    sums_theta = tf.transpose(tf.contrib.kfac.utils.kronecker_product(sums_theta1,tf.ones((num_n, 1))))\n",
    "    \n",
    "    # Didn't need to do the extra transpose \n",
    "    sums_theta1 = tf.divide(1.0,tf.reshape(sums_theta1, [-1]))\n",
    "    \n",
    "    # Calculates the indexes we need from the g array \n",
    "    indices = [[i%num_n,i] for i in range(num_n*num_pn)]\n",
    "\n",
    "    g = tf.divide(-theta, tf.square(sums_theta))\n",
    "    \n",
    "    # The updated weights for the network\n",
    "    updated_weights = tf.add(tf.gather_nd(g, indices), sums_theta1)\n",
    "\n",
    "    # Indicates a sparse tensor \n",
    "    delta = tf.SparseTensor(indices, updated_weights, g.get_shape().as_list())\n",
    "    \n",
    "    g = tf.sparse_add(g,delta)\n",
    "   \n",
    "    g = tf.multiply(g,g1)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the function a numpy function \n",
    "np_sigmoid = np.vectorize(sigmoid)\n",
    "#np_weight_map = np.vectorize(weight_mapping)\n",
    "np_sigmoidGradient = np.vectorize(sigmoidGradient)\n",
    "np_sigmoidn = np.vectorize(sigmoid)\n",
    "np_sigmoidGradientn = np.vectorize(sigmoidGradient)\n",
    "\n",
    "# Make sure the functions are in float32\n",
    "np_sigmoid_32 = lambda x: np_sigmoid(x).astype(np.float32)\n",
    "#np_weight_map_32 = lambda x: np_weight_map(x).astype(np.float32)\n",
    "np_sigmoidGradient_32 = lambda x: np_sigmoidGradient(x).astype(np.float32)\n",
    "np_sigmoidn_32 = lambda x: np_sigmoidn(x).astype(np.float32)\n",
    "np_sigmoidGradientn_32 = lambda x: np_sigmoidGradientn(x).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_sigmoidGradient(x,name=None):\n",
    "    with ops.op_scope([x], name, \"sigmoidGradient\") as name:\n",
    "        y = tf.py_func(np_sigmoidGradient_32,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        stateful=False)\n",
    "        return y[0]\n",
    "    \n",
    "def tf_sigmoidGradientn(x,name=None):\n",
    "    with ops.op_scope([x], name, \"sigmoidGradientn\") as name:\n",
    "        y = tf.py_func(np_sigmoidGradientn_32,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        stateful=False)\n",
    "        return y[0]\n",
    "\n",
    "# Function that tells tensorflow how to calculate the gradients\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
    "\n",
    "    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "def sigmoidGradient(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    n_gr = tf_sigmoidGradient(x)\n",
    "    return grad * n_gr\n",
    "\n",
    "def sigmoidGradientn(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    n_gr = tf_sigmoidGradientn(x)\n",
    "    return grad * n_gr\n",
    "\n",
    "def weightGradient(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    n_gr = weight_md(x)\n",
    "    return grad * n_gr  \n",
    "\n",
    "def tf_sigmoid(x, name=None):\n",
    "    \n",
    "    with tf.name_scope(name, \"sigmoid\", [x]) as name:\n",
    "        y = py_func(np_sigmoid_32,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=sigmoidGradient)  # <-- here's the call to the gradient\n",
    "        return y[0]\n",
    "    \n",
    "def tf_sigmoidn(x, name=None):\n",
    "\n",
    "    with tf.name_scope(name, \"sigmoidn\", [x]) as name:\n",
    "        y = py_func(np_sigmoidn_32,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=sigmoidGradientn)  # <-- here's the call to the gradient\n",
    "        return y[0]\n",
    "    \n",
    "def tf_weight_map(x, name=None):\n",
    "\n",
    "    with tf.name_scope(name, \"weight_mapping\", [x]) as name:\n",
    "        y = py_func(weight_mapping,\n",
    "                        [x],\n",
    "                        [tf.float32],\n",
    "                        name=name,\n",
    "                        grad=weightGradient)  # <-- here's the call to the gradient\n",
    "        return y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that holds all the Hspice params necessary for this network\n",
    "spice_params = {'K': 7.33,\n",
    "               'Vdd': 1.0,\n",
    "               'cc': np.array([-0.0012, -0.2483,  -0.0235,  31.7581]),\n",
    "               'cc2': np.array([-0.0011,  0.2490,  -0.0203, 193.0602]),\n",
    "               'lambda': 0,\n",
    "               'Ron': 100,\n",
    "               'Roff': 16e3\n",
    "               }\n",
    "\n",
    "# Loading the image input files needed to feed into the network\n",
    "train_vals = np.random.permutation(np.arange(1000))[:800] # Line 99: main.m\n",
    "test_vals = np.array([i for i in range(1000) if i not in train_vals])\n",
    "mnist = loadmat('MNIST_complete.mat') # Line 100: main.m\n",
    "\n",
    "X_dat = mnist['activationsPooled'] # Line 101: main.m\n",
    "Y_dat = mnist['y'].transpose() # Line 102: main.m\n",
    "\n",
    "# Values to Train the network\n",
    "X_train = ((X_dat[:,train_vals] - 0.5 ) /2.0).transpose() # Line 103: main.m\n",
    "Y_train = Y_dat[:,train_vals].transpose() # Line 104: main.m\n",
    "\n",
    "#Values to Test the network\n",
    "X_test = ((X_dat[:,test_vals] - 0.5 ) /2.0).transpose() # Line 103: main.m\n",
    "Y_test = Y_dat[:,test_vals].transpose() # Line 104: main.m\n",
    "\n",
    "# Network Parameters\n",
    "input_layer_size = 196 # MNIST data input (img shape: 14*14)\n",
    "num_labels = 10 # MNIST total classes (0-9 digits)\n",
    "num_hidden_layer = 1 # Line 126: main.m (number of hidden layers)\n",
    "hidden_layer_size = 100 # Line 127: main.m (1st layer number of neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 196) (200, 196) (800, 10) (200, 10)\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta1 [100, 197]\n",
      "Theta1n [100, 197]\n",
      "Theta2 [10, 101]\n",
      "Theta2n [10, 101]\n"
     ]
    }
   ],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 196]) # mnist data image of shape 14*14 = 196\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Initializing the Cost J\n",
    "#J = tf.Variable(tf.zeros([1,]))\n",
    "J = 0\n",
    "\n",
    "# Get the number of rows in the fed value at run-time (for our case 800)\n",
    "m = tf.shape(X)[0]\n",
    "\n",
    "# Defines the positive and negative bias terms respectively (pbias,nbias)\n",
    "pbias = tf.fill((m, 1),(spice_params['Vdd']/2.0))\n",
    "nbias = tf.fill((m, 1),(-spice_params['Vdd']/2.0))\n",
    "\n",
    "X1 = tf.concat([pbias, X], 1)\n",
    "X2 = tf.concat([nbias, -X], 1)\n",
    "\n",
    "# # Defines the trainable parameters\n",
    "# theta1 = tf.Variable(tf.zeros([hidden_layer_size, input_layer_size + 1]))\n",
    "# theta1n = tf.Variable(tf.zeros([hidden_layer_size, input_layer_size + 1]))\n",
    "# theta2 = tf.Variable(tf.zeros([num_labels, hidden_layer_size + 1]))\n",
    "# theta2n = tf.Variable(tf.zeros([num_labels, hidden_layer_size + 1]))\n",
    "\n",
    "# Defining the unconstrained weights\n",
    "nn_params = {\n",
    "    'Theta1': tf.Variable(tf.zeros([hidden_layer_size, input_layer_size + 1])),\n",
    "    'Theta1n': tf.Variable(tf.zeros([hidden_layer_size, input_layer_size + 1])),\n",
    "    'Theta2': tf.Variable(tf.zeros([num_labels, hidden_layer_size + 1])),\n",
    "    'Theta2n': tf.Variable(tf.zeros([num_labels, hidden_layer_size + 1])),\n",
    "}\n",
    "for key in nn_params:\n",
    "    print(key, nn_params[key].get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-89a692c87ea7>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializes the delta of weights and biases of network\n",
    "Delta_2 = tf.zeros(tf.shape(nn_params['Theta2']))\n",
    "Delta_1 = tf.zeros(tf.shape(nn_params['Theta1']))\n",
    "delta_3 = tf.zeros([num_labels,1])\n",
    "delta_2 = tf.zeros([hidden_layer_size,1])\n",
    "    \n",
    "# Forward propogating through the network with the mapped constrained weights \n",
    "weights1 = tf_weight_map(tf.concat([nn_params['Theta1'],nn_params['Theta1n']], 1)) # constrained weights\n",
    "    \n",
    "sigma1 = tf.add(tf.matmul(X1,tf.transpose(weights1[:tf.shape(nn_params['Theta1'])[0],:tf.shape(nn_params['Theta1'])[1]])), \\\n",
    "                tf.matmul(X2,tf.transpose(weights1[:tf.shape(nn_params['Theta1'])[0],tf.shape(nn_params['Theta1'])[1]:])))\n",
    "    \n",
    "h1 = tf_sigmoid(sigma1) # Activation values out of hidden layer\n",
    "    \n",
    "weights2 = tf_weight_map(tf.concat([nn_params['Theta2'],nn_params['Theta2n']], 1)) # constrained weights\n",
    "    \n",
    "sigmaO = tf.add(tf.matmul(tf.concat([pbias,h1],1),tf.transpose(weights2[:tf.shape(nn_params['Theta2'])[0],:tf.shape(nn_params['Theta2'])[1]])), \\\n",
    "                tf.matmul(tf.concat([nbias,tf_sigmoidn(sigma1)],1),tf.transpose(weights2[:tf.shape(nn_params['Theta2'])[0],tf.shape(nn_params['Theta2'])[1]:])))\n",
    "  \n",
    "# Activation values out of the output layer\n",
    "h_theta = tf.add(tf_sigmoid(sigmaO), 0.5)\n",
    "\n",
    "# Loss calculated per Arash recommendation\n",
    "J_h = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = h_theta, labels = Y))\n",
    "\n",
    "# Forward Propogation Step\n",
    "a_1 = tf.concat([tf.transpose(pbias), tf.transpose(X)], 0)\n",
    "a_1n = tf.concat([tf.transpose(nbias), tf.transpose(-X)], 0)\n",
    "z_2 = tf.add(tf.matmul(weights1[:tf.shape(nn_params['Theta1'])[0],:tf.shape(nn_params['Theta1'])[1]], a_1),\n",
    "                    tf.matmul(weights1[:tf.shape(nn_params['Theta1'])[0],tf.shape(nn_params['Theta1'])[1]:], a_1n))\n",
    "\n",
    "a_2 = tf.concat([tf.transpose(pbias), tf_sigmoid(z_2)], 0)\n",
    "a_2n = tf.concat([tf.transpose(nbias), tf_sigmoidn(z_2)], 0)\n",
    "z_3 = tf.add(tf.matmul(weights2[:tf.shape(nn_params['Theta2'])[0],:tf.shape(nn_params['Theta2'])[1]],a_2),\n",
    "                    tf.matmul(weights2[:tf.shape(nn_params['Theta2'])[0],tf.shape(nn_params['Theta2'])[1]:],a_2n))\n",
    "#a_3 = tf_sigmoid(z_3)\n",
    "a_3 = tf.add(tf_sigmoid(z_3), 0.5)\n",
    "\n",
    "# Loss calculated per Arash recommendation\n",
    "J_a = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = tf.transpose(a_3), labels = Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above J_a and J_h should have the same value of cost, wondering if we can get away with only calculating one. If you run all the cells besides the two below than you can test this functionality for yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1_md = tf.gradients(weights1, [tf.concat([nn_params['Theta1'], nn_params['Theta1n']], 1)])[0]\n",
    "weights2_md = tf.gradients(weights2, [tf.concat([nn_params['Theta2'], nn_params['Theta2n']], 1)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Propogation Step\n",
    "delta_3 = -tf.subtract(a_3, tf.transpose(Y)) # Gets the difference \n",
    "delta_2 = tf.add(tf.multiply(tf.matmul(tf.transpose(weights2[:tf.shape(nn_params['Theta2'])[0],:tf.shape(nn_params['Theta2'])[1]]), delta_3), \\\n",
    "                                        tf.concat([tf.transpose(pbias), sigmoidGradient(z_2)], 0)),\\\n",
    "                            tf.multiply(tf.matmul(tf.transpose(weights2[:tf.shape(nn_params['Theta2'])[0],:tf.shape(nn_params['Theta2'])[1]]), delta_3), \\\n",
    "                                        tf.concat([tf.transpose(nbias), sigmoidGradientn(z_2)], 0)))    \n",
    "\n",
    "# # Re-Mapping the unconstrianed weights \n",
    "# weights1_md = weight_md(tf.concat([nn_params['Theta1'], nn_params['Theta1n']], 1))\n",
    "# weights2_md = weight_md(tf.concat([nn_params['Theta2'], nn_params['Theta2n']], 1))\n",
    "\n",
    "# Re-Mapping the unconstrianed weights \n",
    "#tf.gradients(y, [x])[0]\n",
    "weights1_md = tf.gradients(weights1, [tf.concat([nn_params['Theta1'], nn_params['Theta1n']], 1)])[0]\n",
    "weights2_md = tf.gradients(weights2, [tf.concat([nn_params['Theta2'], nn_params['Theta2n']], 1)])[0]\n",
    "\n",
    "# Total delta from the 2nd layer of network\n",
    "Delta_2t = tf.matmul(delta_3, tf.matmul(tf.transpose(tf.concat([a_2,a_2n], 0)), weights2_md))\n",
    "\n",
    "# Calculates indices to gather values from total delta of 2nd layer for positive and negative contributions respectively\n",
    "Delta2_pidx = []\n",
    "Delta2_nidx = []\n",
    "Delta2_row = delta_3.get_shape()[0]\n",
    "Delta2_col = nn_params['Theta2'].get_shape().as_list()[1]\n",
    "\n",
    "for r in range(Delta2_row):\n",
    "    #print('Positive Indices -> ROW: %s Column Range: %d - %d' % (r, r*2*col_size, (2*r+1)*col_size))\n",
    "    #print('Negative Indices -> ROW: %s Column Range: %d - %d' % (r, (2*r+1)*col_size, (r+1)*2*col_size))\n",
    "    Delta2_pidx.extend([[r,i] for i in range(r*2*Delta2_col, (2*r+1)*Delta2_col)])\n",
    "    Delta2_nidx.extend([[r,i] for i in range((2*r+1)*Delta2_col, (r+1)*2*Delta2_col)]) \n",
    "\n",
    "# Separates the contributed delta from the positive crossbar and negative crossbar of the 2nd layer\n",
    "Delta_2 = tf.reshape(tf.gather_nd(Delta_2t, Delta2_pidx),[Delta2_row, -1])\n",
    "Delta_2n = tf.reshape(tf.gather_nd(Delta_2t, Delta2_nidx),[Delta2_row, -1])\n",
    "    \n",
    "# Total delta from the 1st layer of network\n",
    "Delta_1t = tf.matmul(delta_2[1:,:],tf.matmul(tf.transpose(tf.concat([a_1,a_1n], 0)), weights1_md))\n",
    "\n",
    "# Calculates indices to gather values from total delta of 1st layer for positive and negative contributions respectively\n",
    "Delta1_pidx = []\n",
    "Delta1_nidx = []\n",
    "Delta1_row = hidden_layer_size\n",
    "Delta1_col = nn_params['Theta1'].get_shape().as_list()[1]\n",
    "\n",
    "for r in range(Delta1_row):\n",
    "    #print('Positive Indices -> ROW: %s Column Range: %d - %d' % (r, r*2*Delta1_col, (2*r+1)*Delta1_col))\n",
    "    #print('Negative Indices -> ROW: %s Column Range: %d - %d' % (r, (2*r+1)*Delta1_col, (r+1)*2*Delta1_col))\n",
    "    Delta1_pidx.extend([[r,i] for i in range(r*2*Delta1_col, (2*r+1)*Delta1_col)])\n",
    "    Delta1_nidx.extend([[r,i] for i in range((2*r+1)*Delta1_col, (r+1)*2*Delta1_col)])\n",
    "    \n",
    "# Separates the contributed delta from the positive crossbar and negative crossbar of the 1st layer\n",
    "Delta_1 = tf.reshape(tf.gather_nd(Delta_1t, Delta1_pidx),[Delta1_row, -1])\n",
    "Delta_1n = tf.reshape(tf.gather_nd(Delta_1t, Delta1_nidx),[Delta1_row, -1]) \n",
    "    \n",
    "# Theta 2 gradient Variables are updated\n",
    "theta_coef = tf.multiply(tf.cast(tf.divide(1,m), dtype = tf.float32), tf.constant(spice_params['cc'][3], dtype = tf.float32))\n",
    "\n",
    "theta2_grad = tf.concat([tf.reshape(tf.multiply(tf.multiply((2.0/spice_params['Vdd']),theta_coef), Delta_2[:,0]), [-1,1]), \\\n",
    "                         tf.add(tf.multiply(tf.multiply((2.0/spice_params['Vdd']),theta_coef), Delta_2[:,1:]), \\\n",
    "                   tf.multiply(nn_params['Theta2'][:,1:], tf.cast(tf.divide(spice_params['lambda'],m), dtype = tf.float32)))], 1)\n",
    "theta2n_grad = tf.concat([tf.reshape(tf.multiply(tf.multiply((2.0/spice_params['Vdd']),theta_coef), Delta_2n[:,0]), [-1,1]), \\\n",
    "                         tf.add(tf.multiply(tf.multiply((2.0/spice_params['Vdd']),theta_coef), Delta_2n[:,1:]), \\\n",
    "                   tf.multiply(nn_params['Theta2n'][:,1:], tf.cast(tf.divide(spice_params['lambda'],m), dtype = tf.float32)))], 1)\n",
    "    \n",
    "    # Theta 1 gradient Variables are updated\n",
    "theta1_grad = tf.concat([tf.reshape(tf.multiply(theta_coef, Delta_1[:,0]), [-1,1]), \\\n",
    "                         tf.add(tf.multiply(theta_coef, Delta_1[:,1:]), \\\n",
    "                   tf.multiply(nn_params['Theta1'][:,1:], tf.cast(tf.divide(spice_params['lambda'],m), dtype = tf.float32)))], 1)\n",
    "theta1n_grad = tf.concat([tf.reshape(tf.multiply(theta_coef, Delta_1n[:,0]), [-1,1]), \\\n",
    "                         tf.add(tf.multiply(theta_coef, Delta_1n[:,1:]), \\\n",
    "                   tf.multiply(nn_params['Theta1n'][:,1:], tf.cast(tf.divide(spice_params['lambda'],m), dtype = tf.float32)))], 1)\n",
    "    \n",
    "return h_theta,[theta1_grad, theta1n_grad, theta2_grad, theta2n_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Initialization of the weights op\n",
    "init_step = [\n",
    "    tf.assign(nn_params['Theta1'], randInitializeWeights(input_layer_size, hidden_layer_size)),\n",
    "    tf.assign(nn_params['Theta1n'], randInitializeWeights(input_layer_size, hidden_layer_size)),\n",
    "    tf.assign(nn_params['Theta2'], randInitializeWeights(hidden_layer_size, num_labels)),\n",
    "    tf.assign(nn_params['Theta2n'], randInitializeWeights(hidden_layer_size, num_labels))  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025827"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(init_step)\n",
    "sess.run(J_h, feed_dict = {X: X_train, Y: Y_train})\n",
    "#x = tf.constant([0.2,0.7,1.2,1.7])\n",
    "#y = tf_spiky(x)\n",
    "#tf.initialize_all_variables().run()\n",
    "\n",
    "#print(x.eval(), y.eval(), tf.gradients(y, [x])[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
